# -*- coding: utf-8 -*-
"""M2M100.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vcBQXn8SHbUTo6T56Mb_ejkR9vMPvaC5

# Import Models
"""

! pip install transformers==4.16.0

! pip install SentencePiece

import pandas as pd
import numpy as np
import torch
import json
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
from transformers import M2M100Config, M2M100ForConditionalGeneration, M2M100Tokenizer

import os
from google.colab import drive
drive.mount('/content/drive')

cur_path = "/content/drive/MyDrive/"

f = open('/content/drive/MyDrive/M2M100/questions21.json',) 
data1 = json.load(f) 
df1 = pd.DataFrame.from_dict(data1, orient="index", columns=[ "question"]).reset_index()
df1 = df1.dropna(axis=0)

f = open('/content/drive/MyDrive/M2M100/Questions.json',) 
data2 = json.load(f) 
questions = [x['Question'] for x in data2]
# field = [x['Field'] for x in data2]
df2 = pd.DataFrame(data={'question': questions})

questions = pd.concat([df1.question, df2.question],ignore_index=True)
df = pd.DataFrame(data={'question': questions})

first_model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
sec_model = M2M100ForConditionalGeneration.from_pretrained("facebook/m2m100_418M")
tokenizer = M2M100Tokenizer.from_pretrained("facebook/m2m100_418M")
first_model.cuda()
sec_model.cuda()

"""# test

"""

test1 = df['question'][7580]
print(test1)

text_to_translate = "Give me all the patients who passed away on 12/27/2020"

def bt(sentence, target_lan, original_lan): 
  sentence = tokenizer(sentence, return_tensors="pt")
  sentence = sentence.to('cuda')
  gen_tokens = first_model.generate(**sentence, forced_bos_token_id=tokenizer.get_lang_id(target_lan))
  out_target = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)
  print(out_target)

  model_inputs = tokenizer(out_target, return_tensors="pt")
  model_inputs = model_inputs.to('cuda')
  gen_tokens = sec_model.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(original_lan))
  out_org = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)
  print(out_org)

bt(test1, 'zh', 'en')

bt(test1, 'ja', 'en')

bt(test1, 'hi', 'en')

bt(test1, 'ru', 'en')

bt(test1, 'ko', 'en')

bt(test1, 'es', 'en')

bt(test1, 'fr', 'en')

bt(test1, 'de', 'en')

"""# Generation"""

def bt_out(data, target_lan, original_lan):
  out_target = []
  for sentence in tqdm(data): 
    sentence = tokenizer(sentence, return_tensors="pt")
    sentence = sentence.to('cuda')
    gen_tokens = first_model.generate(**sentence, forced_bos_token_id=tokenizer.get_lang_id(target_lan))
    out = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)
    out_target.append(out)

  out_orignal = []
  for sentence in tqdm(out_target):
    sentence = tokenizer(sentence, return_tensors="pt")
    sentence = sentence.to('cuda')
    gen_tokens = sec_model.generate(**sentence, forced_bos_token_id=tokenizer.get_lang_id(original_lan))
    out = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)
    out_orignal.append(out)

  return out_target, out_orignal

q_ja, q_ja_en = bt_out(df['question'], 'ja', 'en')
df['q_ja'] = q_ja
df['q_ja_en'] = q_ja_en

df.to_csv('m2m100_out.csv', index=False)

q_zh, q_zh_en = bt_out(df['question'], 'zh', 'en')
df['q_zh'] = q_zh
df['q_zh_en'] = q_zh_en

q_zh, q_zh_en = bt_out(df['question'], 'hi', 'en')
df['q_hi'] = q_zh
df['q_zh_en'] = q_zh_en

"""# Another test on Japanese"""

subset = df['question'].sample(10)
out_test1, out_test2 = bt_out(subset, 'ja', 'en')

for x in range(len(list(subset))):
  print(list(subset)[x])
  print(out_test1[x])
  print(out_test2[x])
  print('-------------------------------------')